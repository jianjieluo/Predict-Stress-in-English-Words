{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考文献：\n",
    "pickle使用: http://www.cnblogs.com/pzxbc/archive/2012/03/18/2404715.html\n",
    "\n",
    "sklearn: http://www.cnblogs.com/jasonfreak/p/5448462.html\n",
    "\n",
    "数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 特征工程学习用例\n",
    "\n",
    "\n",
    "使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明。IRIS数据集由Fisher在1936年整理，包含4个特征（Sepal.Length（花萼长度）、Sepal.Width（花萼宽度）、Petal.Length（花瓣长度）、Petal.Width（花瓣宽度）），特征值都为正浮点数，单位为厘米。目标值为鸢尾花的分类（Iris Setosa（山鸢尾）、Iris Versicolour（杂色鸢尾），Iris Virginica（维吉尼亚鸢尾））。导入IRIS数据集的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "iris = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.  ,   5.1 ,   3.5 ,   1.4 ,   0.2 ,  26.01,  17.85,   7.14,\n",
       "         1.02,  12.25,   4.9 ,   0.7 ,   1.96,   0.28,   0.04])"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PolynomialFeatures().fit_transform(iris.data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L0范数是指向量中非0的元素的个数。\n",
    "\n",
    "L1范数是指向量中各个元素绝对值之和\n",
    "\n",
    "L2范数是指向量各元素的平方和然后求平方根"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 预处理学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "#标准化，返回值为标准化后的数据\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "StandardScaler().fit_transform(iris.data);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-377-265cfb09f05f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-377-265cfb09f05f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    MinMaxScaler().fit_transform(iris.data)；\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#区间缩放，返回值为缩放到[0, 1]区间的数据\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "MinMaxScaler().fit_transform(iris.data)；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化与归一化的区别\n",
    "\n",
    "　　简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为l2的归一化公式如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#归一化，返回值为归一化后的数据\n",
    "from sklearn.preprocessing import Normalizer\n",
    "Normalizer().fit_transform(iris.data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 数据变换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#多项式转换\n",
    "PolynomialFeatures().fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 特征选择\n",
    "\n",
    "特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。\n",
    "\n",
    "特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。\n",
    "\n",
    "**根据特征选择的形式又可以将特征选择方法分为3种：**\n",
    "\n",
    "Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。\n",
    "\n",
    "Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。\n",
    "\n",
    "Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。\n",
    "\n",
    "*使用sklearn中的feature_selection库来进行特征选择*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 作业相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "个人理解：\n",
    "1. 由于数据没给特征矩阵，所以要先自己根据数据构造特征矩阵\n",
    "2. 没有缺失值，维数不大，所以基本不需要降维，规格化也不用，因为特征值都是我们生成的，但预处理需要处理noise之类\n",
    "\n",
    "### 1.特征构造（参考别人的构造先构造一波）\n",
    "1.1. Count of vowels in word\n",
    "\n",
    "1.2. The vowels in word\n",
    "\n",
    "1.3. The phonemes before each vowel \n",
    "\n",
    "1.4. The phonemes after each vowel \n",
    "\n",
    "1.5. prefix in word\n",
    "\n",
    "1.6. suffix in word（验证无用，方差为0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = helper.read_data('./asset/training_data.txt')\n",
    "testing_data = helper.read_data('./asset/tiny_test.txt')\n",
    "#定义元辅音集合\n",
    "vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "\n",
    "#元、辅音合集合\n",
    "vowelAndconsonan = vowel + consonant;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./asset/pd_TrainData.txt', sep=':')\n",
    "df_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#获取前缀后缀, 直接用了别人的\n",
    "def s_has_pre(s):\n",
    "    pre = \"an,dis,in,ig,il,im,ir,ne,n,non,neg,un,male,mal,pseudo,mis,de\\\n",
    "    un,anti,ant,contra,contre,contro,counter,ob,oc,of,op,with,by,circum,\\\n",
    "    circu,de,en,ex,ec,es,fore,in,il,im,ir,inter,intel,intro,medi,med,mid,out,\\\n",
    "    over,post,pre,pro,sub,suc,suf,sug,sup,sur,sus,sur,trans,under,up,\\\n",
    "    ante,anti,ex,fore,mid,medi,post,pre,pri,out,over,post,pre,pro,sub,suc,suf,\\\n",
    "    sug,sum,sup,sur,sus,super,sur,trans,under,up,ante,anti,ex,fore,mid,medi,post,\\\n",
    "    pre,pri,pro,re,by,extra,hyper,out,over,sub,suc,sur,super,sur,under,vice,com,\\\n",
    "    cop,con,cor,co,syn,syl,sym,al,over,pan,ex,for,re,se,dia,per,pel,trans，ad,\\\n",
    "    ac,af,ag,an,ap,ar,as,at,ambi,bin,di,twi,tri,thir,deca,deco,dec,deci,hecto,\\\n",
    "    hect,centi,kilo,myria,mega,micro,multi,poly,hemi,demi,semi,pene,arch,auto,bene,\\\n",
    "    eu,male,mal,macro,magni,micro,aud,bio,ge,phon,tele,\\\n",
    "    ac,ad,af,ag,al,an,ap,as,at,an,ab,abs,acer,acid,acri,act,ag,acu,aer,aero,ag,agi,\\\n",
    "    ig,act,agri,agro,alb,albo,ali,allo,alter,alt,am,ami,amor,ambi,ambul,ana,ano,andr,\\\n",
    "    andro,ang,anim,ann,annu,enni,ante,anthrop,anti,ant,anti,antico,apo,ap,aph,aqu,arch,\\\n",
    "    aster,astr,auc,aug,aut,aud,audi,aur,aus,aug,auc,aut,auto,bar,be,belli,bene,bi,bine,\\\n",
    "    bibl,bibli,biblio,bio,bi,brev,cad,cap,cas,ceiv,cept,capt,cid,cip,cad,cas,calor,capit,\\\n",
    "    capt,carn,cat,cata,cath,caus,caut,cause,cuse,cus,ceas,ced,cede,ceed,cess,cent,centr,\\\n",
    "    centri,chrom,chron,cide,cis,cise,circum,cit,civ,clam,claim,clin,clud,clusclaus,co,cog,\\\n",
    "    col,coll,con,com,cor,cogn,gnos,com,con,contr,contra,counter,cord,cor,cardi,corp,cort,\\\n",
    "    cosm,cour,cur,curr,curs,crat,cracy,cre,cresc,cret,crease,crea,cred,cresc,cret,crease,\\\n",
    "    cru,crit,cur,curs,cura,cycl,cyclo,de,dec,deca,dec,dign,dei,div,dem,demo,dent,dont,derm,\\\n",
    "    di,dy,dia,dic,dict,dit,dis,dif,dit,doc,doct,domin,don,dorm,dox,duc,duct,dura,dynam,dys,\\\n",
    "    ec,eco,ecto,en,em,end,epi,equi,erg,ev,et,ex,exter,extra,extro,fa,fess,fac,fact,fec,fect,\\\n",
    "    fic,fas,fea,fall,fals,femto,fer,fic,feign,fain,fit,feat,fid,fid,fide,feder,fig,fila,fili,\\\n",
    "    fin,fix,flex,flect,flict,flu,fluc,fluv,flux,for,fore,forc,fort,form,fract,frag,frai,fuge,\\\n",
    "    fuse,gam,gastr,gastro,gen,gen,geo,germ,gest,giga,gin,gloss,glot,glu,glo,gor,grad,gress\\\n",
    "    ,gree,graph,gram,graf,grat,grav,greg,hale,heal,helio,hema,hemo,her,here,hes,hetero,hex\\\n",
    "    ,ses,sex,homo,hum,human,hydr,hydra,hydro,hyper,hypn,an,ics,ignis,in,im,in,im,il,ir,infra\\\n",
    "    ,inter,intra,intro,ty,jac,ject,join,junct,judice,jug,junct,just,juven,labor,lau,lav,lot\\\n",
    "    ,lut,lect,leg,lig,leg,levi,lex,leag,leg,liber,liver,lide,liter,loc,loco,log,logo,ology\\\n",
    "    ,loqu,locut,luc,lum,lun,lus,lust,lude,macr,macer,magn,main,mal,man,manu,mand,mania,mar\\\n",
    "    ,mari,mer,matri,medi,mega,mem,ment,meso,meta,meter,metr,micro,migra,mill,kilo,milli,min\\\n",
    "    ,mis,mit,miss,mob,mov,mot,mon,mono,mor,mort,morph,multi,nano,nasc,nat,gnant,nai,nat,nasc\\\n",
    "    ,neo,neur,nom,nom,nym,nomen,nomin,non,non,nov,nox,noc,numer,numisma,ob,oc,of,op,oct,oligo\\\n",
    "    ,omni,onym,oper,ortho,over,pac,pair,pare,paleo,pan,para,pat,pass,path,pater,patr,path,pathy\\\n",
    "    ,ped,pod,pedo,pel,puls,pend,pens,pond,per,peri,phage,phan,phas,phen,fan,phant,fant,phe,phil\\\n",
    "    ,phlegma,phobia,phobos,phon,phot,photo,pico,pict,plac,plais,pli,ply,plore,plu,plur,plus,pneuma\\\n",
    "    ,pneumon,pod,poli,poly,pon,pos,pound,pop,port,portion,post,pot,pre,pur,prehendere,prin,prim,\\\n",
    "    prime,pro,proto,psych,punct,pute,quat,quad,quint,penta,quip,quir,quis,quest,quer,re,reg,recti\\\n",
    "    ,retro,ri,ridi,risi,rog,roga,rupt,sacr,sanc,secr,salv,salu,sanct,sat,satis,sci,scio,scientia,\\\n",
    "    scope,scrib,script,se,sect,sec,sed,sess,sid,semi,sen,scen,sent,sens,sept,sequ,secu,sue,serv,\\\n",
    "    sign,signi,simil,simul,sist,sta,stit,soci,sol,solus,solv,solu,solut,somn,soph,spec,spect,spi,\\\n",
    "    spic,sper,sphere,spir,stand,stant,stab,stat,stan,sti,sta,st,stead,strain,strict,string,stige,\\\n",
    "    stru,struct,stroy,stry,sub,suc,suf,sup,sur,sus,sume,sump,super,supra,syn,sym,tact,tang,tag,tig,\\\n",
    "    ting,tain,ten,tent,tin,tect,teg,tele,tem,tempo,ten,tin,tain,tend,tent,tens,tera,term,terr,terra,\\\n",
    "    test,the,theo,therm,thesis,thet,tire,tom,tor,tors,tort,tox,tract,tra,trai,treat,trans,tri,trib,\\\n",
    "    tribute,turbo,typ,ultima,umber,umbraticum,un,uni,vac,vade,vale,vali,valu,veh,vect,ven,vent,ver,\\\n",
    "    veri,verb,verv,vert,vers,vi,vic,vicis,vict,vinc,vid,vis,viv,vita,vivi,voc,voke,vol,volcan,volv\\\n",
    "    ,volt,vol,vor,with,zo\".replace(\" \",\"\").split(\",\")\n",
    "    for i in pre:\n",
    "        if s.startswith(i.upper()):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# 验证无用\n",
    "def s_has_end(s):\n",
    "    end = \"ee,ese,esque,se,eer,ique,ty,less,ness,ly,ible,able,ion,ic,ical,al,ian,ic,\\\n",
    "    ion,ity,ment,ed,es,er,est,or,ary,ory,ous,cy,ry,ty,al,ure,ute,ble,ar,ly,less,ful,ing,\\\n",
    "    ,inal,tion,sion,osis,oon,sce,\\\n",
    "    que,ette,eer,ee,aire,able,ible,acy,cy,ade,age,al,al,ial,ical,an,ance,ence,ancy,\\\n",
    "    ency,ant,ent,ant,ent,ient,ar,ary,ard,art,ate,ate,ate,ation,cade,drome,ed,ed,en,en,\\\n",
    "    ence,ency,er,ier,er,or,er,or,ery,es,ese,ies,es,ies,ess,est,iest,fold,ful,ful,fy,ia,\\\n",
    "    ian,iatry,ic,ic,ice,ify,ile,ing,ion,ish,ism,ist,ite,ity,ive,ive,ative,itive,ize,less,\\\n",
    "    ly,ment,ness,or,ory,ous,eous,ose,ious,ship,ster,ure,ward,wise,ize,phy,ogy,ity,ion,ic,ical,al\".replace(\" \",\"\").split(\",\")\n",
    "    for i in end:\n",
    "        if s.endswith(i.upper()):\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#输入训练单词和音标获取：目标值，元音数, 音标序列，元音序列, 有无对应前缀，有无对应后缀\n",
    "def getInfoOfPronsFromTrain(word, prons):\n",
    "    hasPre = s_has_pre(word)\n",
    "    hasEnd = s_has_end(word)\n",
    "    pronsSeq = []\n",
    "    vowelsSeq = []\n",
    "    stressVowelIndex = 0\n",
    "    count = 0\n",
    "    sub_prons = prons.split(' ')\n",
    "    for i in range(len(sub_prons)):\n",
    "        if sub_prons[i][-1].isdigit():\n",
    "            vowelsSeq.append(vowel.index(sub_prons[i][0:-1])) #取得元音在元音集合中的序号\n",
    "            pronsSeq.append(vowelAndconsonan.index(sub_prons[i][0:-1])) #取得元音在元、辅音集合中的序号\n",
    "            count+=1\n",
    "            if sub_prons[i][-1] == '1':\n",
    "                stressVowelIndex = count\n",
    "        else:\n",
    "            pronsSeq.append(vowelAndconsonan.index(sub_prons[i])) #取得辅音在元辅、音集合中的序号\n",
    "    return stressVowelIndex, count, pronsSeq, vowelsSeq, hasPre, hasEnd\n",
    "\n",
    "#输入测试单词和目标获取：元音数, 音标序列，元音序列, 有无对应前缀，有无对应后缀\n",
    "def getInfoOfPronsFromTest(word, prons):\n",
    "    hasPre = s_has_pre(word)\n",
    "    hasEnd = s_has_end(word)\n",
    "    pronsSeq = []\n",
    "    vowelsSeq = []\n",
    "    count = 0\n",
    "    sub_prons = prons.split(' ')\n",
    "    for i in range(len(sub_prons)):\n",
    "        if sub_prons[i] in voewl:\n",
    "            vowelsSeq.append(vowel.index(sub_prons[i])) #取得元音在元音集合中的序号\n",
    "            count+=1\n",
    "        pronsSeq.append(vowelAndconsonan.index(sub_prons[i])) #取得辅音在元辅、音集合中的序号\n",
    "    return stressVowelIndex, count, pronsSeq, vowelsSeq, hasPre, hasEnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "a1, a2 = training_data[3].split(':')\n",
    "print(a1, a2)\n",
    "b1, b2, b3, b4, b5, b6 = getInfoOfPronsFromTrain(a1, a2)\n",
    "print(b1, b2, b3, b4, b5, b6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetArr = []\n",
    "countArr = []\n",
    "pronsSeqArr = [] \n",
    "vowelsSeqArr = []\n",
    "hasPreArr = []\n",
    "hasEndArr = []\n",
    "\n",
    "for i in range(len(training_data)):\n",
    "    t_word, t_prons = training_data[i].split(':')\n",
    "    t1, t2, t3, t4, t5, t6 = getInfoOfPronsFromTrain(t_word, t_prons)\n",
    "    targetArr.append(t1)\n",
    "    countArr.append(t2)\n",
    "    pronsSeqArr.append(t3) \n",
    "    vowelsSeqArr.append(t4)\n",
    "    hasPreArr.append(t5)\n",
    "    hasEndArr.append(t6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'data' : training_data,\n",
    "                   'target' : targetArr,\n",
    "                   'count' : countArr,\n",
    "                   'pronsSeq' : pronsSeqArr,\n",
    "                   'vowelsSeq' : vowelsSeqArr,\n",
    "                   'hasPre' : hasPreArr,\n",
    "                   'hasEnd' : hasEndArr\n",
    "})\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronsSeqArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "研究发现hasEnd的方差为0，舍去"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.groupby(['count', 'target']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造特征矩阵0\n",
    "labels_0 = []\n",
    "for i in range(len(training_data)):\n",
    "    labels_0.append([countArr[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构造特征矩阵1\n",
    "labels_1 = []\n",
    "for i in range(len(training_data)):\n",
    "    labels_1.append([countArr[i], hasPreArr[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(labels_1, targetArr, test_size = 0.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 决策树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#使用信息熵作为划分标准，对决策树进行训练\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "#系数反映每个特征的影响力。越大表示该特征在分类中起到的作用越大\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = clf.predict(x_train)\n",
    "answer2 = clf.predict(x_test)\n",
    "print(answer)\n",
    "# print(np.mean( answer == y_train))\n",
    "print('f1 for train = ' , f1_score(y_train, answer, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, answer2, average='micro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 贝叶斯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "clf_bayes = linear_model.BayesianRidge()\n",
    "clf_bayes.fit(x_train, y_train)\n",
    "answer = clf_bayes.predict(x_train).round() #取整\n",
    "answer2 = clf_bayes.predict(x_test).round()\n",
    "print('f1 for train = ' , f1_score(y_train, answer, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, answer2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
