{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 各种分类器\n",
    "\n",
    "\n",
    "如果把机器学习归为两大类，那么主要的工作可以分为：分类和聚类。而分类任务基本上占整个机器学习或者是数据挖掘领域的70%,可见我们遇到的很多问题，都可以用分类的算法进行解决。机器学习发展到现在，许多被证实有效的分类算法被提出，例如我们经常会用到的K-近邻分类器、朴素贝叶斯分类器、支持向量机(SVM)、决策树算法等。大家平时在用的时候可能并不太清楚每种分类算法适合哪种类型的数据，因为对于不同的数据集，上述算法的效果可能有很大的区别，所以了解每种分类器的特点对于解决实际问题有很大的帮助。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "## 训练集有多大？\n",
    "如果你的训练集很小，高偏差/低方差的分类器（如朴素贝叶斯）比低偏差/高方差的分类器（如K近邻或Logistic回归）更有优势，因为后者容易过拟合。但是随着训练集的增大，高偏差的分类器并不能训练出非常准确的模型，所以低偏差/高方差的分类器会胜出（它们有更小的渐近误差）。你也可以从生成模型与鉴别模型的区别来考虑它们。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各种参考网站：\n",
    "\n",
    "自己实现分类器（不用库）： http://blog.csdn.net/column/details/top10-algorithm-dm.html\n",
    "\n",
    "https://www.zhihu.com/question/24169940/answer/26952728\n",
    "\n",
    "分类器的使用（库）：http://blog.csdn.net/lsldd/article/details/41223147/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.决策树（Decision Tree, DT）\n",
    "\n",
    "DT容易理解与解释（对某些人而言——不确定我是否也在他们其中）。DT是非参数的，所以你不需要担心野点（或离群点）和数据是否线性可分的问题（例如，DT可以轻松的处理这种情况：属于A类的样本的特征x取值往往非常小或者非常大，而属于B类的样本的特征x取值在中间范围）。DT的主要缺点是容易过拟合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import f1_score \n",
    "\n",
    "#一般有entropy和gini两种系数\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.朴素贝叶斯(Naive Bayes, NB)\n",
    "\n",
    "超级简单，就像做一些数数的工作。如果条件独立假设成立的话，NB将比鉴别模型（如Logistic回归）收敛的更快，所以你只需要少量的训练数据。即使条件独立假设不成立，NB在实际中仍然表现出惊人的好。如果你想做类似半监督学习，或者是既要模型简单又要性能好，NB值得尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这个是贝叶斯线性回归\n",
    "from sklearn import linear_model\n",
    "clf_bayes = linear_model.BayesianRidge()\n",
    "clf_bayes.fit(x_train, y_train)\n",
    "answer = clf_bayes.predict(x_train).round() #取整\n",
    "answer2 = clf_bayes.predict(x_test).round()\n",
    "print('f1 for train = ' , f1_score(y_train, answer, average='micro'))\n",
    "print('f1 for test = ' , f1_score(y_test, answer2, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "#调用MultinomialNB分类器\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf = GaussianNB()\n",
    "clf = BernoulliNB()\n",
    "#doc_class_predicted = clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另外还有其他的朴素贝叶斯分类器如GaussianNB适用于高斯分布（正态分布）的特征，而BernoulliNB适用于伯努利分布（二值分布）的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3.Logistic回归(Logistic Regression, LR)\n",
    "\n",
    "LR有很多方法来对模型正则化。比起NB的条件独立性假设，LR不需要考虑样本是否是相关的。与决策树与支持向量机（SVM）不同，NB有很好的概率解释，且很容易利用新的训练数据来更新模型（使用在线梯度下降法）。如果你想要一些概率信息（如，为了更容易的调整分类阈值，得到分类的不确定性，得到置信区间），或者希望将来有更多数据时能方便的更新改进模型，LR是值得使用的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#加载数据集，切分数据集80%训练，20%测试\n",
    "x_train, x_test, y_train, y_test\\\n",
    "= train_test_split(movie_data, movie_target, test_size = 0.2)\n",
    "   \n",
    "\n",
    "    #训练LR分类器\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.KNN分类算法\n",
    "\n",
    "KNN分类算法（K-Nearest-Neighbors Classification），又叫K近邻算法，是一个概念极其简单，而分类效果又很优秀的分类算法。\n",
    "\n",
    "他的核心思想就是，要确定测试样本属于哪一类，就寻找所有训练样本中与该测试样本“距离”最近的前K个样本，然后看这K个样本大部分属于哪一类，那么就认为这个测试样本也属于哪一类。简单的说就是让最相似的K个样本来投票决定。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "clf = neighbors.KNeighborsClassifier(algorithm='kd_tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.K均值聚类\n",
    "\n",
    "K-Means算法思想简单，效果却很好，是最有名的聚类算法。聚类算法的步骤如下：\n",
    "\n",
    "1：初始化K个样本作为初始聚类中心；\n",
    "\n",
    "2：计算每个样本点到K个中心的距离，选择最近的中心作为其分类，直到所有样本点分类完毕；\n",
    "\n",
    "3：分别计算K个类中所有样本的质心，作为新的中心点，完成一轮迭代。\n",
    "\n",
    "通常的迭代结束条件为新的质心与之前的质心偏移值小于一个给定阈值。\n",
    "\n",
    "\n",
    "老师上课讲过，但貌似这里不太适合用？个人不是很懂，望赐教。\n",
    "\n",
    "但这个写得挺有趣 http://www.cnblogs.com/leoo2sk/archive/2010/09/20/k-means.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
