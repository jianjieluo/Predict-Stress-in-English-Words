{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树算法：http://blog.csdn.net/lsldd/article/details/41223147/\n",
    "\n",
    "其他参考：https://github.com/richardyoungsyd/machine_learning_on_word_stress_prediction/blob/master/submission.py\n",
    "        https://github.com/uygnef/COMP9318/tree/master/proj_spec\n",
    "\n",
    "我个人实在没有太好的想法，但我感觉利用元音个数，重音位置，前后缀这些组合也是足够好的特征值\n",
    "\n",
    "兆丰研究做得很好，我就只是把剑杰给的GitHub的源码做了点注释，有一些小小改动。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\administrator\\appdata\\local\\programs\\python\\python35-32\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#同前一样\n",
    "def get_Vocab(s):\n",
    "    return s.split(\":\")\n",
    "def s_has_pre(s):\n",
    "    pre = \"an,dis,in,ig,il,im,ir,ne,n,non,neg,un,male,mal,pseudo,mis,de\\\n",
    "    un,anti,ant,contra,contre,contro,counter,ob,oc,of,op,with,by,circum,\\\n",
    "    circu,de,en,ex,ec,es,fore,in,il,im,ir,inter,intel,intro,medi,med,mid,out,\\\n",
    "    over,post,pre,pro,sub,suc,suf,sug,sup,sur,sus,sur,trans,under,up,\\\n",
    "    ante,anti,ex,fore,mid,medi,post,pre,pri,out,over,post,pre,pro,sub,suc,suf,\\\n",
    "    sug,sum,sup,sur,sus,super,sur,trans,under,up,ante,anti,ex,fore,mid,medi,post,\\\n",
    "    pre,pri,pro,re,by,extra,hyper,out,over,sub,suc,sur,super,sur,under,vice,com,\\\n",
    "    cop,con,cor,co,syn,syl,sym,al,over,pan,ex,for,re,se,dia,per,pel,trans，ad,\\\n",
    "    ac,af,ag,an,ap,ar,as,at,ambi,bin,di,twi,tri,thir,deca,deco,dec,deci,hecto,\\\n",
    "    hect,centi,kilo,myria,mega,micro,multi,poly,hemi,demi,semi,pene,arch,auto,bene,\\\n",
    "    eu,male,mal,macro,magni,micro,aud,bio,ge,phon,tele,\\\n",
    "    ac,ad,af,ag,al,an,ap,as,at,an,ab,abs,acer,acid,acri,act,ag,acu,aer,aero,ag,agi,\\\n",
    "    ig,act,agri,agro,alb,albo,ali,allo,alter,alt,am,ami,amor,ambi,ambul,ana,ano,andr,\\\n",
    "    andro,ang,anim,ann,annu,enni,ante,anthrop,anti,ant,anti,antico,apo,ap,aph,aqu,arch,\\\n",
    "    aster,astr,auc,aug,aut,aud,audi,aur,aus,aug,auc,aut,auto,bar,be,belli,bene,bi,bine,\\\n",
    "    bibl,bibli,biblio,bio,bi,brev,cad,cap,cas,ceiv,cept,capt,cid,cip,cad,cas,calor,capit,\\\n",
    "    capt,carn,cat,cata,cath,caus,caut,cause,cuse,cus,ceas,ced,cede,ceed,cess,cent,centr,\\\n",
    "    centri,chrom,chron,cide,cis,cise,circum,cit,civ,clam,claim,clin,clud,clusclaus,co,cog,\\\n",
    "    col,coll,con,com,cor,cogn,gnos,com,con,contr,contra,counter,cord,cor,cardi,corp,cort,\\\n",
    "    cosm,cour,cur,curr,curs,crat,cracy,cre,cresc,cret,crease,crea,cred,cresc,cret,crease,\\\n",
    "    cru,crit,cur,curs,cura,cycl,cyclo,de,dec,deca,dec,dign,dei,div,dem,demo,dent,dont,derm,\\\n",
    "    di,dy,dia,dic,dict,dit,dis,dif,dit,doc,doct,domin,don,dorm,dox,duc,duct,dura,dynam,dys,\\\n",
    "    ec,eco,ecto,en,em,end,epi,equi,erg,ev,et,ex,exter,extra,extro,fa,fess,fac,fact,fec,fect,\\\n",
    "    fic,fas,fea,fall,fals,femto,fer,fic,feign,fain,fit,feat,fid,fid,fide,feder,fig,fila,fili,\\\n",
    "    fin,fix,flex,flect,flict,flu,fluc,fluv,flux,for,fore,forc,fort,form,fract,frag,frai,fuge,\\\n",
    "    fuse,gam,gastr,gastro,gen,gen,geo,germ,gest,giga,gin,gloss,glot,glu,glo,gor,grad,gress\\\n",
    "    ,gree,graph,gram,graf,grat,grav,greg,hale,heal,helio,hema,hemo,her,here,hes,hetero,hex\\\n",
    "    ,ses,sex,homo,hum,human,hydr,hydra,hydro,hyper,hypn,an,ics,ignis,in,im,in,im,il,ir,infra\\\n",
    "    ,inter,intra,intro,ty,jac,ject,join,junct,judice,jug,junct,just,juven,labor,lau,lav,lot\\\n",
    "    ,lut,lect,leg,lig,leg,levi,lex,leag,leg,liber,liver,lide,liter,loc,loco,log,logo,ology\\\n",
    "    ,loqu,locut,luc,lum,lun,lus,lust,lude,macr,macer,magn,main,mal,man,manu,mand,mania,mar\\\n",
    "    ,mari,mer,matri,medi,mega,mem,ment,meso,meta,meter,metr,micro,migra,mill,kilo,milli,min\\\n",
    "    ,mis,mit,miss,mob,mov,mot,mon,mono,mor,mort,morph,multi,nano,nasc,nat,gnant,nai,nat,nasc\\\n",
    "    ,neo,neur,nom,nom,nym,nomen,nomin,non,non,nov,nox,noc,numer,numisma,ob,oc,of,op,oct,oligo\\\n",
    "    ,omni,onym,oper,ortho,over,pac,pair,pare,paleo,pan,para,pat,pass,path,pater,patr,path,pathy\\\n",
    "    ,ped,pod,pedo,pel,puls,pend,pens,pond,per,peri,phage,phan,phas,phen,fan,phant,fant,phe,phil\\\n",
    "    ,phlegma,phobia,phobos,phon,phot,photo,pico,pict,plac,plais,pli,ply,plore,plu,plur,plus,pneuma\\\n",
    "    ,pneumon,pod,poli,poly,pon,pos,pound,pop,port,portion,post,pot,pre,pur,prehendere,prin,prim,\\\n",
    "    prime,pro,proto,psych,punct,pute,quat,quad,quint,penta,quip,quir,quis,quest,quer,re,reg,recti\\\n",
    "    ,retro,ri,ridi,risi,rog,roga,rupt,sacr,sanc,secr,salv,salu,sanct,sat,satis,sci,scio,scientia,\\\n",
    "    scope,scrib,script,se,sect,sec,sed,sess,sid,semi,sen,scen,sent,sens,sept,sequ,secu,sue,serv,\\\n",
    "    sign,signi,simil,simul,sist,sta,stit,soci,sol,solus,solv,solu,solut,somn,soph,spec,spect,spi,\\\n",
    "    spic,sper,sphere,spir,stand,stant,stab,stat,stan,sti,sta,st,stead,strain,strict,string,stige,\\\n",
    "    stru,struct,stroy,stry,sub,suc,suf,sup,sur,sus,sume,sump,super,supra,syn,sym,tact,tang,tag,tig,\\\n",
    "    ting,tain,ten,tent,tin,tect,teg,tele,tem,tempo,ten,tin,tain,tend,tent,tens,tera,term,terr,terra,\\\n",
    "    test,the,theo,therm,thesis,thet,tire,tom,tor,tors,tort,tox,tract,tra,trai,treat,trans,tri,trib,\\\n",
    "    tribute,turbo,typ,ultima,umber,umbraticum,un,uni,vac,vade,vale,vali,valu,veh,vect,ven,vent,ver,\\\n",
    "    veri,verb,verv,vert,vers,vi,vic,vicis,vict,vinc,vid,vis,viv,vita,vivi,voc,voke,vol,volcan,volv\\\n",
    "    ,volt,vol,vor,with,zo\".replace(\" \",\"\").split(\",\")\n",
    "\n",
    "    for i in pre:\n",
    "        if s.startswith(i.upper()):\n",
    "            return 1\n",
    "    return 0\n",
    "def s_has_end(s):\n",
    "    end = \"ee,ese,esque,se,eer,ique,ty,less,ness,ly,ible,able,ion,ic,ical,al,ian,ic,\\\n",
    "    ion,ity,ment,ed,es,er,est,or,ary,ory,ous,cy,ry,ty,al,ure,ute,ble,ar,ly,less,ful,ing,\\\n",
    "    ,inal,tion,sion,osis,oon,sce,\\\n",
    "    que,ette,eer,ee,aire,able,ible,acy,cy,ade,age,al,al,ial,ical,an,ance,ence,ancy,\\\n",
    "    ency,ant,ent,ant,ent,ient,ar,ary,ard,art,ate,ate,ate,ation,cade,drome,ed,ed,en,en,\\\n",
    "    ence,ency,er,ier,er,or,er,or,ery,es,ese,ies,es,ies,ess,est,iest,fold,ful,ful,fy,ia,\\\n",
    "    ian,iatry,ic,ic,ice,ify,ile,ing,ion,ish,ism,ist,ite,ity,ive,ive,ative,itive,ize,less,\\\n",
    "    ly,ment,ness,or,ory,ous,eous,ose,ious,ship,ster,ure,ward,wise,ize,phy,ogy,ity,ion,ic,ical,al\".replace(\" \",\"\").split(\",\")\n",
    "    for i in end:\n",
    "        if s.endswith(i.upper()):\n",
    "            return 1\n",
    "    return 0\n",
    "def rip_number(s):\n",
    "    if s[-1].isdigit():\n",
    "        s = s[:-1]\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_Slice(s,r,vowel,cons):\n",
    "    vocab,phonemes = get_Vocab(s)\n",
    "    #print(vocab)\n",
    "    phonemes = phonemes.split(\" \")#音标简化\n",
    "    stress_matrix = []#重音矩阵\n",
    "    words = vowel+cons#不懂\n",
    "    has_pre = 0#是否有前后缀\n",
    "    has_end = 0\n",
    "    #A SLICE OF FEATURE\n",
    "    #1.音节数量 2.音节组合 5.是否有前缀 6.后缀是否为这些 7.prime\n",
    "    current = []#不懂\n",
    "    vowels = [-1,-1,-1,-1]#元音\n",
    "    vowel_pos = [-1,-1,-1,-1,-1,-1,-1,-1]#元音位置\n",
    "    count_vowel = 0#帮助计数\n",
    "\n",
    "    has_pre = s_has_pre(vocab)\n",
    "    has_end = s_has_end(vocab)#是否有前后缀\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(len(phonemes)):\n",
    "        if phonemes[i][-1].isdigit():#是元音\n",
    "            stress_matrix.append(phonemes[i][-1])\n",
    "            vowels[count_vowel]=vowel.index(phonemes[i][:-1])#元音\n",
    "            if i>0:#音标的位置\n",
    "                vowel_pos[2*count_vowel] = words.index(rip_number(phonemes[i-1]))#不懂为什么是vowel_pos[]里面是2*..\n",
    "            if i+1<len(phonemes):\n",
    "                vowel_pos[1+2*count_vowel] = words.index(rip_number(phonemes[i+1]))#同上不懂\n",
    "            count_vowel+=1#用来帮助元音计数\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "    if \"2\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"2\")    \n",
    "    elif \"1\" in stress_matrix:\n",
    "        prime_pos = stress_matrix.index(\"1\")#重音矩阵有什么值，prime_pos为重音在哪\n",
    "        \n",
    "\n",
    "\n",
    "    r.append([len(stress_matrix)]+vowels+vowel_pos+[vocab,has_pre,has_end,prime_pos+1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_Slice2(s,r,vowel,cons):#因为是用来test的，所以并不知道真正的重音在哪，没有prime_pos\n",
    "    vocab,phonemes = get_Vocab(s)\n",
    "    phonemes = phonemes.split(\" \")#音标简化\n",
    "    words = vowel+cons#不懂\n",
    "    #A SLICE OF FEATURE, 1.NUMBER_OF_VOW 2.NUMBER_OF_syllables 3.VOWEL1_pos 4.VOWEL2_pos 5.VOWEL3_pos 6.VOWEL1\n",
    "    #7VOWEL2 8.VOWEL3 9.p_s\n",
    "    current = []#不懂\n",
    "    count_vowel = 0#帮助计数\n",
    "    vowels = [-1,-1,-1,-1]#，元音\n",
    "    vowel_pos = [-1,-1,-1,-1,-1,-1,-1,-1]#元音位置\n",
    "\n",
    "\n",
    "    for i in range(len(phonemes)):\n",
    "        if phonemes[i] in vowel: #是元音\n",
    "            vowels[count_vowel]=vowel.index(phonemes[i])#直接带上\n",
    "            if i>0:\n",
    "                vowel_pos[2*count_vowel] = words.index(phonemes[i-1])\n",
    "            if i+1<len(phonemes):\n",
    "                vowel_pos[1+2*count_vowel] = words.index(phonemes[i+1])\n",
    "            count_vowel+=1#其实和上面的len(stress_matrix)一样\n",
    "\n",
    "    has_pre = s_has_pre(vocab)\n",
    "    has_end = s_has_end(vocab)\n",
    "\n",
    "    r.append([count_vowel]+vowels+vowel_pos+[vocab,has_pre,has_end])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_Inf(s):\n",
    "    dic = {}\n",
    "    r = []\n",
    "    vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "    consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "    for i in s:\n",
    "        get_Slice(i,r,vowel,consonant)#传值进去\n",
    "    features_and_label = pd.DataFrame(r)\n",
    "    return features_and_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_Inf2(s):\n",
    "    dic = {}\n",
    "    r = []\n",
    "    vowel = \"AA, AE, AH, AO, AW, AY, EH, ER, EY, IH, IY, OW, OY, UH, UW\".replace(\",\",\"\").split()\n",
    "    consonant = \"P, B, CH, D, DH, F, G, HH, JH, K, L, M, N, NG, R, S, SH, T, TH, V, W, Y, Z, ZH\".replace(\",\",\"\").split()\n",
    "    for i in s:\n",
    "        get_Slice2(i,r,vowel,consonant)\n",
    "    features_and_label = pd.DataFrame(r)\n",
    "\n",
    "    return features_and_label\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_type(s):\n",
    "    types = \"CC,CD,DT,EX,FW,IN,JJ,JJR,JJS,LS,MD,NN,NNS,NNP,NNPS,PDT,POS,PRP,PRP$,RB,RBR,RBS,RP,SYM,TO\\\n",
    "    UH,VB,VBD,VBG,VBN,VBP,VBZ,WDT,WP,WP$,WRB\".split(\",\")#好像是nltk的用来标记文本中的成分\n",
    "    type_list = []\n",
    "\n",
    "    for i in range (len(s)):\n",
    "        word_type = nltk.pos_tag([s[i].capitalize()])#标记文本中成分\n",
    "        type_list.append(types.index(word_type[0][1]))#\n",
    "\n",
    "    return type_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################# training #################\n",
    "\n",
    "def train(data, classifier_file):\n",
    "\n",
    "    features_and_label = get_Inf(data)\n",
    "    feature_list = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "    features_and_label.loc[:,13] = get_type(features_and_label.loc[:,13])#特征值\n",
    "\n",
    "\n",
    "    X_train = features_and_label[feature_list]\n",
    "    y_train = features_and_label[16]\n",
    "\n",
    "    clf = DecisionTreeClassifier(criterion = \"entropy\")#书上有用gini也有用entropy的\n",
    "    dtree = clf.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "    print(dtree.score(X_train,y_train))\n",
    "    output = open('classifier_file', 'wb')#写回去\n",
    "    pickle.dump(clf, output)\n",
    "    output.close()\n",
    "    return y_train    \n",
    "\n",
    "################# testing #################\n",
    "\n",
    "\n",
    "def test(data, classifier_file):\n",
    "    pkl_file = open('classifier_file', 'rb')\n",
    "    dt = pickle.load(pkl_file)\n",
    "    r = []\n",
    "    features_and_label = get_Inf2(data)\n",
    "    features_and_label[13] = get_type(features_and_label[13])\n",
    "    r = dt.predict(features_and_label)#predict\n",
    "    for i in range (len(r)):\n",
    "        if r[i]==0:\n",
    "            r[i]=1\n",
    "\n",
    "    pkl_file.close()\n",
    "    return list(r)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
